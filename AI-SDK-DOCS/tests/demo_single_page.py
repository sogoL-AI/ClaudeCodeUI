#!/usr/bin/env python3

"""
å•é¡µé¢æ¼”ç¤ºè„šæœ¬

ä½¿ç”¨æ–°çš„åŒ…ç»“æ„æ¼”ç¤ºå•é¡µé¢çˆ¬å–åŠŸèƒ½ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
src_path = project_root / 'src'
sys.path.insert(0, str(src_path))

from src.scraper import AISDKDocsScraper, ScraperConfig
from src.utils import setup_logging, get_logger


def demo_single_page_scraping():
    """æ¼”ç¤ºå•é¡µé¢çˆ¬å–"""
    
    print("ğŸš€ AI SDK å•é¡µé¢çˆ¬å–æ¼”ç¤º")
    print("=" * 50)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(level='INFO')
    logger = get_logger(__name__)
    
    try:
        # åˆ›å»ºé…ç½®ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£ä»¥ä¾¿è§‚å¯Ÿï¼‰
        config = ScraperConfig()
        config.HEADLESS = False  # æ˜¾ç¤ºæµè§ˆå™¨
        
        # åˆ›å»ºçˆ¬å–å™¨
        logger.info("åˆ›å»ºçˆ¬å–å™¨...")
        scraper = AISDKDocsScraper(config)
        
        # æ˜¾ç¤ºå½“å‰è¿›åº¦
        progress = scraper.get_progress_info()
        logger.info(f"å½“å‰è¿›åº¦: {progress['scraped_pages']}/{progress['total_pages']} ({progress['progress_percentage']:.1f}%)")
        
        # é€‰æ‹©è¦æ¼”ç¤ºçš„é¡µé¢
        demo_pages = ['introduction', 'foundations', 'ai-sdk-core']
        
        for page_key in demo_pages:
            if page_key in scraper.status_data['pages']:
                page_data = scraper.status_data['pages'][page_key]
                
                logger.info(f"\nğŸ“ æ¼”ç¤ºé¡µé¢: {page_data['title']}")
                logger.info(f"ğŸ”— URL: {config.BASE_URL}{page_data['url']}")
                
                if page_data['scraped']:
                    logger.info("âœ… è¯¥é¡µé¢å·²ç»çˆ¬å–è¿‡ï¼Œè·³è¿‡æ¼”ç¤º")
                    continue
                
                # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦çˆ¬å–è¿™ä¸ªé¡µé¢
                user_input = input(f"æ˜¯å¦è¦çˆ¬å–é¡µé¢ '{page_data['title']}'? (y/n): ").lower().strip()
                
                if user_input == 'y':
                    logger.info("å¼€å§‹çˆ¬å–...")
                    success = scraper.scrape_single_page(page_key)
                    
                    if success:
                        logger.info("ğŸ‰ çˆ¬å–æˆåŠŸ!")
                        
                        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯
                        if page_data.get('filePath'):
                            file_path = project_root / page_data['filePath']
                            if file_path.exists():
                                file_size = file_path.stat().st_size
                                logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
                                logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                                
                                # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹é¢„è§ˆ
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                    preview = content[:300] + "..." if len(content) > 300 else content
                                    logger.info(f"ğŸ“„ å†…å®¹é¢„è§ˆ:\n{preview}")
                    else:
                        logger.error("âŒ çˆ¬å–å¤±è´¥")
                
                else:
                    logger.info("è·³è¿‡æ­¤é¡µé¢")
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                if input("\nç»§ç»­æ¼”ç¤ºä¸‹ä¸€ä¸ªé¡µé¢? (y/n): ").lower().strip() != 'y':
                    break
            else:
                logger.warning(f"é¡µé¢ '{page_key}' ä¸å­˜åœ¨äºçŠ¶æ€æ–‡ä»¶ä¸­")
        
        # æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
        final_progress = scraper.get_progress_info()
        logger.info(f"\næœ€ç»ˆè¿›åº¦: {final_progress['scraped_pages']}/{final_progress['total_pages']} ({final_progress['progress_percentage']:.1f}%)")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­äº†æ¼”ç¤º")
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ æ¼”ç¤ºå®Œæˆ!")


def list_available_pages():
    """åˆ—å‡ºå¯ç”¨é¡µé¢"""
    print("\nğŸ“‹ å¯ç”¨é¡µé¢åˆ—è¡¨:")
    print("-" * 50)
    
    try:
        scraper = AISDKDocsScraper()
        
        for category in ['root', 'foundations', 'getting-started', 'ai-sdk-core', 'ai-sdk-ui']:
            print(f"\nğŸ“‚ {category.upper()}:")
            
            category_pages = [
                (key, data) for key, data in scraper.status_data['pages'].items() 
                if data.get('category') == category
            ]
            
            for page_key, page_data in category_pages[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                status = "âœ…" if page_data.get('scraped') else "â³"
                print(f"  {status} {page_key}: {page_data.get('title', 'Unknown')}")
            
            if len(category_pages) > 5:
                print(f"  ... è¿˜æœ‰ {len(category_pages) - 5} ä¸ªé¡µé¢")
    
    except Exception as e:
        print(f"æ— æ³•åŠ è½½é¡µé¢åˆ—è¡¨: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. äº¤äº’å¼å•é¡µé¢çˆ¬å–æ¼”ç¤º")
    print("2. åˆ—å‡ºæ‰€æœ‰å¯ç”¨é¡µé¢")
    print("3. é€€å‡º")
    
    while True:
        choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
        
        if choice == '1':
            demo_single_page_scraping()
            break
        elif choice == '2':
            list_available_pages()
            break
        elif choice == '3':
            print("å†è§!")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é€‰æ‹© 1-3")


if __name__ == "__main__":
    main()